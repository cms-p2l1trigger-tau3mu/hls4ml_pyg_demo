{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Please follow the instructions on README.mk file for installing the necessary packages to run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b78f5f",
   "metadata": {},
   "source": [
    "This walkthrough has few instructions. It's mainly just code to help the user to understand the pytorch geometric to hls4ml pipeline. If there's any confusion, please email me at yun79@purdue.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1907e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95ed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e745ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n",
      "handler args: ('NodeEncoderBatchNorm1d',)\n",
      "handler args: ('EdgeEncoderBatchNorm1d',)\n",
      "handler args: ('MeanPool',)\n",
      "handler args: ('fc_out',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "import hls4ml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.bv_interaction_network_pyg import BvGENConvBig\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size\n",
    "import time\n",
    "import pickle as pkl\n",
    "from Tau3MuGNNs.src.models.bv_model import convertBnToBvbn\n",
    "\n",
    "from Tau3MuGNNs.src.utils import Criterion, Tau3MuDataset, log_epoch, load_checkpoint, get_data_loaders, add_cuts_to_config\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eadb9fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tau3MuGNNs.src.utils.dataset.Tau3MuDataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tau3MuDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24082fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We intialize our custom pytorch geometric(pyg) model\n",
    "\"\"\"\n",
    "config = {\n",
    "    \"bn_input\": True,                 # Batch normalization on input features? This is to normalize the input features\n",
    "  \"n_layers\": 8    ,                # Number of GNN layers\n",
    "  \"out_channels\": 128  ,            # Number of hidden channels for each GNN layer\n",
    "  \"dropout_p\": 0  ,               # Dropout probability\n",
    "  \"readout\": \"pool\"  ,                # Specify the method to use for the readout layer. One can also use 'lstm', 'vn' or 'jknet'\n",
    "  \"norm_type\": \"batch\"   ,            # Specify the type of normalization to use. One can also use 'instance', 'layer' or 'graph'\n",
    "  \"deepgcn_aggr\": \"sum\"  ,        # Aggregation function for the DeeperGCN layers. Please refer to their documentation for more details\n",
    "  \"linear_ap_fixed_int\": 4,\n",
    "  \"linear_ap_fixed_fract\": 4,\n",
    "  \"norm_ap_fixed_int\": 4 ,\n",
    "  \"norm_ap_fixed_fract\": 4 ,\n",
    "}\n",
    "\n",
    "bebugging = True\n",
    "\n",
    "intermediary_model = BvGENConvBig(\n",
    "    config[\"n_layers\"], \n",
    "    flow = \"source_to_target\",\n",
    "    out_channels = 128,\n",
    "    int_bitwidth = config[\"norm_ap_fixed_int\"],\n",
    "    fractional_bitwidth = config[\"norm_ap_fixed_fract\"],\n",
    "    debugging = bebugging\n",
    ").eval() # eval mode for bathnorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d894cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_model_path = \"Tau3MuGNNs/data/logs/12_15_2022_14_04_19-GNN_full_test/model.pt\"\n",
    "# state_dict = torch.load(saved_model_path, map_location=\"cpu\")\n",
    "# model_state_dict = state_dict[\"model_state_dict\"]\n",
    "# print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24445b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bv model is being used\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n",
      "self.eps: 1e-07\n",
      "scale: 0.0625\n",
      "min_val: -8.0\n",
      "max_val: 7.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "check whether the state dict loads well\n",
    "on the bv model\n",
    "\"\"\"\n",
    "from Tau3MuGNNs.src.models import BV_Model\n",
    "import torch\n",
    "\n",
    "\n",
    "x_dim = 3\n",
    "edge_attr_dim = 4\n",
    "\n",
    "bv_model = BV_Model(x_dim, edge_attr_dim, True, config, debugging = bebugging).eval()\n",
    "bv_model = convertBnToBvbn(bv_model)\n",
    "\n",
    "\"\"\"\n",
    "We obtain the state dict(trained parameters) from a saved pt file\n",
    "\"\"\"\n",
    "# saved_model_path = \"./model.pt\"\n",
    "saved_model_path = \"Tau3MuGNNs/data/logs/12_15_2022_14_04_19-GNN_full_test/model.pt\"\n",
    "state_dict = torch.load(saved_model_path, map_location=\"cpu\")\n",
    "bv_model.load_state_dict(state_dict['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602c6952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# intermediary_model.node_encoder_norm.state_dict()\n",
    "node_encoder_norm = intermediary_model.node_encoder_norm\n",
    "print(node_encoder_norm.norm.weight)\n",
    "print(node_encoder_norm.norm.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8afdf902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1dToQuantScaleBias\n"
     ]
    }
   ],
   "source": [
    "# print(type(bv_model.mlps[0][1]) == nn.BatchNorm1d)\n",
    "print(bv_model.mlps[0][1].__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f99373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_weight = bv_model.mlps[0][1].quant_weight() \n",
    "weight = bv_model.mlps[0][1].weight\n",
    "# print(bv_model.mlps[0][1].quant_bias() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93853c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# load/transfer the state dict into our pyg model\n",
    "# \"\"\"\n",
    "# model_state_dict = state_dict[\"model_state_dict\"]\n",
    "# # equate the encoders's parameters\n",
    "# intermediary_model.node_encoder.weight = nn.Parameter(model_state_dict['node_encoder.weight'])\n",
    "# intermediary_model.node_encoder.bias = nn.Parameter(model_state_dict['node_encoder.bias'])\n",
    "# intermediary_model.edge_encoder.weight = nn.Parameter(model_state_dict['edge_encoder.weight'])\n",
    "# intermediary_model.edge_encoder.bias = nn.Parameter(model_state_dict['edge_encoder.bias'])\n",
    "\n",
    "# # load the quant batchnorm\n",
    "# # add a new class variable weight for the converter\n",
    "# intermediary_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "#     model_state_dict['bn_edge_feature.weight']\n",
    "# )\n",
    "# # equate the new clss variable with the variable that\n",
    "# # actually matters when forwarding\n",
    "# intermediary_model.edge_encoder_norm.norm.weight = intermediary_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# # repeat with the rest of the batchnorms\n",
    "# intermediary_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "#     model_state_dict['bn_edge_feature.bias']\n",
    "# )\n",
    "# intermediary_model.edge_encoder_norm.norm.bias = intermediary_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "# n_layers = config[\"n_layers\"]\n",
    "# # now the nodeblocks and betas\n",
    "# original_layer_idxs = [0,1,3] # we skip idx=2 bc that's activation\n",
    "# new_layer_mlp_idxs = [0,1,3] \n",
    "# Betas = []\n",
    "# for nodeblock_idx in range(n_layers):\n",
    "#     gnn = intermediary_model.gnns[nodeblock_idx]\n",
    "#     # gnn.beta = model_state_dict[f'convs.{nodeblock_idx}.t']\n",
    "#     # Betas.append(float(gnn.beta[0]))\n",
    "    \n",
    "#     mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    \n",
    "#     for idx in range(len(original_layer_idxs)):\n",
    "#         original_layer_idx = original_layer_idxs[idx]\n",
    "#         new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "#         nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "#         nodeblock = getattr(intermediary_model, nodeblock_name)\n",
    "#         module = nodeblock.layers[new_layer_mlp_idx]\n",
    "#         if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1dToQuantScaleBias'):\n",
    "#             module.weight = nn.Parameter(\n",
    "#                 model_state_dict[mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "#             )\n",
    "#             module.bias = nn.Parameter(\n",
    "#                 model_state_dict[mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "#             )\n",
    "#         else:\n",
    "#             print(f\"Unsupported layer: {module.__class__.__name__}\")\n",
    "        \n",
    "    \n",
    "# intermediary_model.fc_out.weight = nn.Parameter(model_state_dict['fc_out.weight'])\n",
    "# intermediary_model.fc_out.bias = nn.Parameter(model_state_dict['fc_out.bias'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load/transfer the state dict into our pyg model\n",
    "\"\"\"\n",
    "model_state_dict = state_dict[\"model_state_dict\"]\n",
    "# equate the encoders's parameters\n",
    "# for linear layers, we should put in the quant weights\n",
    "intermediary_model.node_encoder.weight = nn.Parameter(\n",
    "    bv_model.node_encoder.quant_weight().value\n",
    ")\n",
    "intermediary_model.node_encoder.bias = nn.Parameter(\n",
    "    bv_model.node_encoder.quant_bias().value\n",
    ")\n",
    "intermediary_model.edge_encoder.weight = nn.Parameter(\n",
    "    bv_model.edge_encoder.quant_weight().value\n",
    ")\n",
    "intermediary_model.edge_encoder.bias = nn.Parameter(\n",
    "    bv_model.edge_encoder.quant_bias().value\n",
    ")\n",
    "\n",
    "# load the quant batchnorm\n",
    "# # add a new class variable weight for the converter\n",
    "# intermediary_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "#     model_state_dict['bn_edge_feature.weight']\n",
    "# )\n",
    "# # equate the new clss variable with the variable that\n",
    "# # actually matters when forwarding\n",
    "# intermediary_model.edge_encoder_norm.norm.weight = intermediary_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# # repeat with the rest of the batchnorms\n",
    "# intermediary_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "#     model_state_dict['bn_edge_feature.bias']\n",
    "# )\n",
    "# intermediary_model.edge_encoder_norm.norm.bias = intermediary_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "intermediary_model.node_encoder_norm.norm = bv_model.bn_node_feature\n",
    "intermediary_model.node_encoder_norm.weight = nn.Parameter(\n",
    "    intermediary_model.node_encoder_norm.norm.quant_weight().value\n",
    ")\n",
    "intermediary_model.node_encoder_norm.bias = nn.Parameter(\n",
    "    intermediary_model.node_encoder_norm.norm.quant_bias().value\n",
    ")\n",
    "\n",
    "intermediary_model.edge_encoder_norm.norm = bv_model.bn_edge_feature\n",
    "intermediary_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "    intermediary_model.edge_encoder_norm.norm.quant_weight().value\n",
    ")\n",
    "intermediary_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "    intermediary_model.edge_encoder_norm.norm.quant_bias().value\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "n_layers = config[\"n_layers\"]\n",
    "# now the nodeblocks and betas\n",
    "original_layer_idxs = [0,1,3] # we skip idx=2 bc that's activation\n",
    "new_layer_mlp_idxs = [0,1,3] \n",
    "Betas = []\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = intermediary_model.gnns[nodeblock_idx]\n",
    "    # gnn.beta = model_state_dict[f'convs.{nodeblock_idx}.t']\n",
    "    # Betas.append(float(gnn.beta[0]))\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    \n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(intermediary_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        bv_module = bv_model.mlps[nodeblock_idx][original_layer_idx]\n",
    "        if (module.__class__.__name__ == 'Linear'):\n",
    "            module.weight = nn.Parameter( \n",
    "                bv_module.quant_weight().value\n",
    "            )\n",
    "            module.bias = nn.Parameter( \n",
    "                bv_module.quant_bias().value\n",
    "            )\n",
    "        elif (module.__class__.__name__ == 'BatchNorm1dToQuantScaleBias'):\n",
    "            # replace module with bv_module\n",
    "            nodeblock.layers[new_layer_mlp_idx] = bv_module\n",
    "        else:\n",
    "            print(f\"Unsupported layer: {module.__class__.__name__}\")\n",
    "        \n",
    "    \n",
    "intermediary_model.fc_out.weight = nn.Parameter(model_state_dict['fc_out.weight'])\n",
    "intermediary_model.fc_out.bias = nn.Parameter(model_state_dict['fc_out.bias'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb1207b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommenting this block for now bc IDK what this is for\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Commenting this block for now bc IDK what this is for\n",
    "\"\"\"\n",
    "# batchnorm_st_dict = OrderedDict()\n",
    "# batchnorm_st_dict[\"weight\"] = state_dict['model_state_dict']['bn_node_feature.weight']\n",
    "# batchnorm_st_dict[\"bias\"] = state_dict['model_state_dict']['bn_node_feature.bias']\n",
    "# intermediary_model.node_encoder_norm.norm.load_state_dict(batchnorm_st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d178043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediary_model.node_encoder_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a122813a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear weight loading for nodeblock 0 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 0 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 0 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 0 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 0 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 0 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 1 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 1 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 1 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 1 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 1 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 1 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 2 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 2 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 2 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 2 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 2 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 2 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 3 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 3 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 3 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 3 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 3 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 3 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 4 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 4 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 4 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 4 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 4 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 4 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 5 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 5 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 5 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 5 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 5 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 5 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 6 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 6 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 6 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 6 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 6 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 6 layer 2 successful: True\n",
      "Linear weight loading for nodeblock 7 layer 0 successful: True\n",
      "Linear bias loading for nodeblock 7 layer 0 successful: True\n",
      "BatchNorm1dToQuantScaleBias weight loading for nodeblock 7 layer 1 successful: True\n",
      "BatchNorm1dToQuantScaleBias bias loading for nodeblock 7 layer 1 successful: True\n",
      "Linear weight loading for nodeblock 7 layer 2 successful: True\n",
      "Linear bias loading for nodeblock 7 layer 2 successful: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just some code to test if the transfer was successful\n",
    "NOTE: This is not a good test, since we still\n",
    "have to calculate the prediction MSEs of the \n",
    "two models to be 100 percent sure\n",
    "\"\"\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = intermediary_model.gnns[nodeblock_idx]\n",
    "    # boolean_val = gnn.beta == state_dict['model_state_dict'][f'convs.{nodeblock_idx}.t']\n",
    "#     print(f\"beta: {gnn.beta}\")\n",
    "    # print(f\"beta loading for layer {idx} successful: {boolean_val}\")\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(intermediary_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        bv_module = bv_model.mlps[nodeblock_idx][original_layer_idx]\n",
    "        if (module.__class__.__name__ == 'Linear'):\n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"weight\"] == bv_module.quant_weight().value\n",
    "            )\n",
    "            print(f\"Linear weight loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"bias\"] == bv_module.quant_bias().value\n",
    "            )\n",
    "            print(f\"Linear bias loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "        elif (module.__class__.__name__ == 'BatchNorm1dToQuantScaleBias'):\n",
    "            weight = module.quant_weight().value\n",
    "            bv_weight = bv_module.quant_weight().value \n",
    "            boolean_val = torch.all(\n",
    "                module.quant_weight().value == bv_module.quant_weight().value            \n",
    "            )\n",
    "            print(f\"BatchNorm1dToQuantScaleBias weight loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.quant_bias().value == bv_module.quant_bias().value\n",
    "            )\n",
    "            print(f\"BatchNorm1dToQuantScaleBias bias loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39cc622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n",
      "test: True\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(8., device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "bv_model.to(device)\n",
    "intermediary_model.to(device)\n",
    "bool_val = intermediary_model.node_encoder.weight == bv_model.node_encoder.quant_weight().value\n",
    "print(torch.all(bool_val))\n",
    "bool_val = intermediary_model.node_encoder.bias == bv_model.node_encoder.quant_bias().value\n",
    "print(torch.all(bool_val))\n",
    "x = torch.rand((4,3)).cuda()\n",
    "intermediary_model.eval()\n",
    "bv_model.eval()\n",
    "# print(intermediary_model.node_encoder.weight.shape)\n",
    "bool_val = intermediary_model.node_encoder(x) == bv_model.node_encoder(x)\n",
    "print(torch.all(bool_val))\n",
    "weight = bv_model.node_encoder.quant_weight().value\n",
    "bias = bv_model.node_encoder.quant_bias().value\n",
    "# weight = bv_model.node_encoder.weight\n",
    "# bias = bv_model.node_encoder.bias\n",
    "quantizer = bv_model.node_encoder.output_quant\n",
    "in_quant = bv_model.node_encoder.input_quant\n",
    "bool_val = bv_model.node_encoder(x).value == quantizer(in_quant(x)@weight.T + bias).value\n",
    "print(f\"test: {torch.all(bool_val)}\")\n",
    "weight = intermediary_model.node_encoder.weight\n",
    "bias = intermediary_model.node_encoder.bias\n",
    "bool_val = intermediary_model.node_encoder(x) == x@weight.T + bias\n",
    "print(torch.all(bool_val))\n",
    "# print(bv_model.node_encoder.weight_quant.bit_width)\n",
    "print(bv_model.node_encoder(x).bit_width)\n",
    "weight = bv_model.node_encoder.quant_weight().value\n",
    "bias = bv_model.node_encoder.quant_bias().value\n",
    "bool_val = intermediary_model.node_encoder(x) == x@weight.T + bias\n",
    "print(torch.all(bool_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9fd2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "# setting = \"GNN_full_test\"\n",
    "# cut_id = None\n",
    "# config_path = \"Tau3MuGNNs/data/logs/11_30_2022_15_26_06-GNN_full_test/config.yml\"\n",
    "# config = yaml.safe_load(Path(config_path).open('r'))\n",
    "# config = add_cuts_to_config(config, cut_id)\n",
    "# # dataset = Tau3MuDataset(setting, config[\"data\"])\n",
    "# data_loaders, x_dim, edge_attr_dim, _ = get_data_loaders(setting, config['data'], config['optimizer']['batch_size'])\n",
    "\n",
    "with open('Tau3MuGNNs/src/data_loaders.pkl', 'rb') as file:\n",
    "    data_loaders = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7399fd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch_geometric.loader.dataloader.DataLoader at 0x7f6a2bf24390>,\n",
       " 'valid': <torch_geometric.loader.dataloader.DataLoader at 0x7f6a23756ad0>,\n",
       " 'test': <torch_geometric.loader.dataloader.DataLoader at 0x7f6a23756950>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066ca44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/71226 [00:02<5:42:39,  3.46it/s]\n",
      "  0%|          | 10/175113 [00:03<14:44:59,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n",
      "torch.Size([20, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bv_model.eval()\n",
    "intermediary_model.eval()\n",
    "device = \"cuda\"\n",
    "bv_model.to(device)\n",
    "intermediary_model.to(device)\n",
    "all_clf_logits, all_clf_labels = [], []\n",
    "intermediary_all_clf_logits = []\n",
    "data_types = [ \"valid\", \"test\"]\n",
    "for data_type in data_types:\n",
    "    data_loader = data_loaders[data_type]\n",
    "    loader_len = len(data_loader)\n",
    "    pbar = tqdm(data_loader, total=loader_len)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        if idx == 10:\n",
    "            break\n",
    "        data.to(device)\n",
    "        y = data.y\n",
    "        # print(torch.all(y == y.data))\n",
    "        clf_logits = bv_model(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch=data.batch, data=data)\n",
    "        intermediary_model_clf_logits = intermediary_model(data)\n",
    "\n",
    "\n",
    "        # .data.cpu() to prevent gpu memory overload\n",
    "        clf_logits=clf_logits.data.cpu()\n",
    "        y = data.y.data.cpu()\n",
    "        intermediary_model_clf_logits=intermediary_model_clf_logits.data.cpu()\n",
    "        # append the prediction logits and labels\n",
    "        all_clf_logits.append(clf_logits) \n",
    "        all_clf_labels.append(y)\n",
    "        intermediary_all_clf_logits.append(intermediary_model_clf_logits)\n",
    "        # print(intermediary_model_clf_logits.shape)\n",
    "        # print(clf_logits.shape)\n",
    "\n",
    "all_clf_logits, all_clf_labels = torch.cat(all_clf_logits), torch.cat(all_clf_labels)\n",
    "intermediary_all_clf_logits = torch.cat(intermediary_all_clf_logits)\n",
    "print(intermediary_all_clf_logits.shape)\n",
    "print(all_clf_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0644b979",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1074442/960130899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mintermediary_all_clf_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediary_all_clf_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_clf_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_clf_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediary_all_clf_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_clf_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediary_all_clf_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_clf_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "intermediary_all_clf_logits = intermediary_all_clf_logits.numpy()\n",
    "all_clf_logits = all_clf_logits.numpy()\n",
    "print(intermediary_all_clf_logits.shape)\n",
    "print(all_clf_logits.shape)\n",
    "MSE = mean_squared_error(intermediary_all_clf_logits, all_clf_logits)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1400f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get some predictions and test to see if they match\n",
    "\"\"\"\n",
    "\n",
    "with open('test_data.pickle', 'rb') as f:\n",
    "    graphs= pkl.load(f) \n",
    "\n",
    "MSEs = []\n",
    "for data in graphs:\n",
    "    intermediary_pred = intermediary_model(\n",
    "                x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    intermediary_pred = intermediary_pred.detach().cpu().numpy().flatten()\n",
    "    bv_pred = bv_model(\n",
    "                x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    bv_pred = bv_pred.detach().cpu().numpy().flatten()\n",
    "    MSE = mean_squared_error(intermediary_pred, bv_pred)\n",
    "    MSEs.append(MSE)\n",
    "\n",
    "    print(f\"torch vs siqi: {mean_squared_error(intermediary_pred, siqi_pred)}\")\n",
    "    \n",
    "print(f\"MSEs: \\n {MSEs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497dd99",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "\"\"\"\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "forward_dict[\"node_encoder_norm\"] = \"NodeEncoderBatchNorm1d\"\n",
    "forward_dict[\"edge_encoder_norm\"] = \"EdgeEncoderBatchNorm1d\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    forward_dict[f\"O_{nodeblock_idx}\"] = \"NodeBlock\"\n",
    "forward_dict[\"pool\"] = \"MeanPool\"  \n",
    "forward_dict[\"fc_out\"] = \"fc_out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf43159",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we define additional parameters.\n",
    "\"\"\"\n",
    "common_dim = 128\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}\n",
    "\n",
    "misc_config = {\"Betas\" : Betas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediary_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943157a",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa1957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We initialize hls model from pyg model\n",
    "\"\"\"\n",
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(intermediary_model,\n",
    "                                   default_precision=\"ap_fixed<52,20>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(intermediary_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       misc_config = misc_config,\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid', #sigmoid\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08573dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(hls_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c28847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "#         graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f42455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hls4ml.model.profiling import numerical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plots = numerical(model=intermediary_model, hls_model = hls_model, X=graphs[0])\n",
    "# plots = numerical(model=intermediary_model)\n",
    "plots = numerical(hls_model = hls_model)\n",
    "# plots = numerical(hls_model = hls_model, X=graphs[0].hls_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0701c",
   "metadata": {},
   "source": [
    "## hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da705cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls_model.compile()\n",
    "# hls_model.build()\n",
    "\"\"\"\n",
    "compile\n",
    "build\n",
    "implementation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "#         graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we are testing hls model output compared to pyg model.\n",
    "We are using Mean Squared Error (MSE) to calculate the differences \n",
    "in the output of the two models.\n",
    "\"\"\"\n",
    "MSE_l = []\n",
    "batch = None\n",
    "siqi_data = None\n",
    "for data in graphs:\n",
    "    intermediary_pred = intermediary_model(data)\n",
    "    intermediary_pred = intermediary_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    siqi_pred = model_siqi(\n",
    "        x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    siqi_pred = siqi_pred.detach().cpu().numpy().flatten()\n",
    "    print(f\"intermediary_pred.shape: {intermediary_pred.shape}\")\n",
    "    print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(intermediary_pred, hls_pred)\n",
    "#     print(np.testing.assert_almost_equal(intermediary_pred, hls_pred))\n",
    "    print(f\"torch vs siqi: {mean_squared_error(intermediary_pred, siqi_pred)}\")\n",
    "    MSE_l.append(MSE)\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l: {MSE_l}\")\n",
    "print(f\"Mean of all MSEs: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1428f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('test_data.pickle', 'rb') as f:\n",
    "    graphs= pkl.load(f) \n",
    "\n",
    "MSEs = []\n",
    "for data in graphs:\n",
    "    intermediary_pred = intermediary_model(data)\n",
    "    intermediary_pred = intermediary_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    MSE = mean_squared_error(intermediary_pred, hls_pred)\n",
    "    MSEs.append(MSE)\n",
    "    siqi_pred = model_siqi(\n",
    "        x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    siqi_pred = siqi_pred.detach().cpu().numpy().flatten()\n",
    "    print(f\"torch vs siqi: {mean_squared_error(intermediary_pred, siqi_pred)}\")\n",
    "    \n",
    "print(f\"MSEs: \\n {MSEs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's load some of tau3mu data from our group (Prof Mia Liu).\n",
    "This is still a smaller sample of the total data, but it's good enough. \n",
    "\n",
    "NOTE: this will take some time (<5mins)\n",
    "\"\"\"\n",
    "import timeit\n",
    "\n",
    "MSEs = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "# turn off debugging here\n",
    "intermediary_model.SetDebugMode(False)\n",
    "\n",
    "for stage in stages:\n",
    "    with open(f'tau3mu_data/test_BIG_data_{stage}.pickle', 'rb') as f:\n",
    "        graphs= pkl.load(f) \n",
    "        \n",
    "    counter = 0\n",
    "    start = timeit.default_timer()\n",
    "    for data in graphs:\n",
    "        # use counter to just keep track of the progress. Nothing fancy\n",
    "        if counter%500 ==0 and counter != 0:\n",
    "            print(f\"counter: {counter}\")\n",
    "        counter += 1\n",
    "        intermediary_pred = intermediary_model(data)\n",
    "        intermediary_pred = intermediary_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(intermediary_pred, hls_pred)\n",
    "        MSEs.append(MSE)\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"time taken: {(end - start)/ 60} mins\")\n",
    "MSEs = np.array(MSEs)\n",
    "print(f\"MSE means: {np.mean(MSEs)}\")\n",
    "print(f\"MSE max: {np.max(MSEs)}\")\n",
    "print(f\"n_total: {MSEs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's graph the MSE distribution\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_total = MSEs.shape[0]\n",
    "mean_val = np.mean(MSEs)\n",
    "\n",
    "plt.hist(MSEs, density=True, bins=50, label=f\"Mean value: {mean_val}\\n max val outlier removed\") \n",
    "plt.ylabel('Occurrence')\n",
    "plt.xlabel('MSE');\n",
    "plt.title(f'MSE of hls vs torch prediction (n_total: {n_total})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('MSEs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e149761",
   "metadata": {},
   "source": [
    "You can see from the graph above that the error is very small (order of magnitude -7). This will obviously get bigger once you use more realistic ap_fixed parameters, but this proves that the conversion itself is working as intended.\n",
    "\n",
    "So this is the latest progress on the pyg to hls conversion. The current model is only one layer out of eight pyg layers from the original Siqi's model. More work is on the way, but hopefully this gives you a good idea of how the conversion pipeline works. \n",
    "\n",
    "For any questions, please email me at yun@purdue.edu, or slack if you already have me on it.\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53405494",
   "metadata": {},
   "source": [
    "# Biography\n",
    "This walkthrough and other local files were taken from Mr Abd Elabd's code at https://github.com/abdelabd/manual_GNN_conversion <br />\n",
    "The hls4ml pyg support's starting code has been taken from Mr. Abd Elabd and Prof Javier Duarte's work: https://github.com/fastmachinelearning/hls4ml/tree/pyg_to_hls_rebase_w_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94335a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7122f7567b8da691c559ecf82c59435132e70cd34ec59ce5f537f2f68228d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
